model_name_or_path: "E:/R1/TheBloke/Llama-2-7B-Chat-GGUF"
model_type: "LLaMA"
dataset: "custom"
dataset_dir: "data"

output_dir: "outputs/lora-test-optimized-v3"
logging_dir: "logs"

do_train: true
overwrite_cache: false
overwrite_output_dir: true

# CPU优化的批次处理
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
gradient_checkpointing: true
gradient_clipping: 1.0

# CPU优化的学习率策略
learning_rate: 5e-5
max_steps: 2000
warmup_steps: 200
logging_steps: 10
save_steps: 100
save_total_limit: 5

# CPU优化的LoRA设置
lora_rank: 32
lora_alpha: 128
lora_dropout: 0.1
weight_decay: 0.01
lora_target: "q_proj,v_proj,k_proj,o_proj"

# 使用简单的余弦调度，减少计算开销
lr_scheduler_type: "cosine"

# 增加序列长度以适应更长的中文输入
max_source_length: 1024
max_target_length: 1024
preprocessing_num_workers: 4

# 启用混合精度训练
bf16: true
